{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"../checkpoints/sam2_hiera_large.pt\"\n",
    "model_cfg = \"sam2_hiera_l.yaml\"\n",
    "\n",
    "sam2_model = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "frames_generator = sv.get_video_frames_generator(\"./videos/src_video/first_video_pre_ethics.mp4\")\n",
    "sink = sv.ImageSink(\n",
    "    target_dir_path=\"./videos/icsi_sv/\",\n",
    "    image_name_pattern=\"{:05d}.jpeg\")\n",
    "\n",
    "with sink:\n",
    "    for frame in frames_generator:\n",
    "        sink.save_image(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 2703/2703 [01:24<00:00, 32.13it/s]\n"
     ]
    }
   ],
   "source": [
    "inference_state = sam2_model.init_state(\"./videos/icsi_sv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n",
    "\n",
    "# Let's add a 2nd positive click at (x, y) = (250, 220) to refine the mask\n",
    "# sending all clicks (and their labels) to `add_new_points_or_box`\n",
    "points = np.array([[700, 600], [800, 600]], dtype=np.float32)\n",
    "# for labels, `1` means positive click and `0` means negative click\n",
    "labels = np.array([1, 1], np.int32)\n",
    "_, out_obj_ids, out_mask_logits = sam2_model.add_new_points_or_box(\n",
    "    inference_state=inference_state,\n",
    "    frame_idx=ann_frame_idx,\n",
    "    obj_id=ann_obj_id,\n",
    "    points=points,\n",
    "    labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@4392.531] global cap.cpp:643 open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.10.0) /io/opencv/modules/videoio/src/cap_images.cpp:430: error: (-215:Assertion failed) !filename_pattern.empty() in function 'open'\n",
      "\n",
      "\n",
      "propagate in video:  26%|██▌       | 692/2703 [03:10<08:09,  4.11it/s]"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "colors = ['#FF1493', '#00BFFF', '#FF6347', '#FFD700']\n",
    "mask_annotator = sv.MaskAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(colors),\n",
    "    color_lookup=sv.ColorLookup.TRACK)\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(\"./videos/src_video/first_video_pre_ethics.mp4\")\n",
    "frames_paths = sorted(sv.list_files_with_extensions(\n",
    "    directory=\"./videos/icsi_sv/\", \n",
    "    extensions=[\"jpeg\"]))\n",
    "\n",
    "with sv.VideoSink(\"./videos/icsi_test_sv\", video_info=video_info) as sink:\n",
    "    for frame_idx, object_ids, mask_logits in sam2_model.propagate_in_video(inference_state):\n",
    "        frame = cv2.imread(frames_paths[frame_idx])\n",
    "        masks = (mask_logits > 0.0).cpu().numpy()\n",
    "        N, X, H, W = masks.shape\n",
    "        masks = masks.reshape(N * X, H, W)\n",
    "        detections = sv.Detections(\n",
    "            xyxy=sv.mask_to_xyxy(masks=masks),\n",
    "            mask=masks,\n",
    "            tracker_id=np.array(object_ids)\n",
    "        )\n",
    "        frame = mask_annotator.annotate(frame, detections)\n",
    "        # cv2.imshow(\"image\",frame)\n",
    "        sink.write_frame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<supervision.utils.video.VideoSink at 0x7f8fe7c7bd70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
